{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 0-shot Seq2Seq evaluation\n","\n","This is a notebook focused on evaluation the Seq2Seq model from CodeT5+ checkpoint with no fine-tuning on our datasets."]},{"cell_type":"markdown","metadata":{},"source":["## Clone the repository"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-27T13:50:21.850728Z","iopub.status.busy":"2023-11-27T13:50:21.850365Z","iopub.status.idle":"2023-11-27T13:50:31.582583Z","shell.execute_reply":"2023-11-27T13:50:31.581520Z","shell.execute_reply.started":"2023-11-27T13:50:21.850697Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'CodeSearcher'...\n","remote: Enumerating objects: 428, done.\u001b[K\n","remote: Counting objects: 100% (68/68), done.\u001b[K\n","remote: Compressing objects: 100% (63/63), done.\u001b[K\n","remote: Total 428 (delta 7), reused 24 (delta 4), pack-reused 360\u001b[K\n","Receiving objects: 100% (428/428), 45.29 MiB | 6.44 MiB/s, done.\n","Resolving deltas: 100% (222/222), done.\n","Branch 'dev/embeddings' set up to track remote branch 'dev/embeddings' from 'origin'.\n","Switched to a new branch 'dev/embeddings'\n"]}],"source":["!git clone https://github.com/leiluk1/CodeSearcher.git && cd CodeSearcher/ && git checkout dev/embeddings"]},{"cell_type":"markdown","metadata":{},"source":["## Set up the required dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install dataprep gdown py7zr transformers peft evaluate rouge_score fire loguru --quiet"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T13:51:30.358890Z","iopub.status.busy":"2023-11-27T13:51:30.358596Z","iopub.status.idle":"2023-11-27T13:51:49.986047Z","shell.execute_reply":"2023-11-27T13:51:49.984867Z","shell.execute_reply.started":"2023-11-27T13:51:30.358860Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading...\n","From (uriginal): https://drive.google.com/uc?id=1tZfsYQgWmc2gG340ru5VbrZ5aLIZ41_6\n","From (redirected): https://drive.google.com/uc?id=1tZfsYQgWmc2gG340ru5VbrZ5aLIZ41_6&confirm=t&uuid=69fab87a-ba5d-42d6-bc15-591b4b06bf2d\n","To: /kaggle/working/XLCoST_data.zip\n","100%|█████████████████████████████████████████| 298M/298M [00:01<00:00, 262MB/s]\n"]}],"source":["!mkdir -p /kaggle/output/CodeSearcher/output\n","!mkdir -p CodeSearcher/data/raw\n","!gdown 1tZfsYQgWmc2gG340ru5VbrZ5aLIZ41_6\n","!unzip -q -d /kaggle/working/CodeSearcher/data/raw ./XLCoST_data.zip"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate the pre-trained Seq2Seq CodeT5+ model on our datasets."]},{"cell_type":"markdown","metadata":{},"source":["### Seq2Seq 0-shot for C++ programming language dataset."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T13:53:24.513589Z","iopub.status.busy":"2023-11-27T13:53:24.513151Z","iopub.status.idle":"2023-11-27T13:55:32.727425Z","shell.execute_reply":"2023-11-27T13:55:32.726440Z","shell.execute_reply.started":"2023-11-27T13:53:24.513530Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","Downloading config.json: 100%|█████████████| 1.07k/1.07k [00:00<00:00, 5.64MB/s]\n","Downloading (…)n_codet5p_bimodal.py: 100%|█| 2.81k/2.81k [00:00<00:00, 17.6MB/s]\n","A new version of the following files was downloaded from https://huggingface.co/Salesforce/codet5p-220m-bimodal:\n","- configuration_codet5p_bimodal.py\n",". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","Downloading (…)g_codet5p_bimodal.py: 100%|█████| 939/939 [00:00<00:00, 4.86MB/s]\n","A new version of the following files was downloaded from https://huggingface.co/Salesforce/codet5p-220m-bimodal:\n","- modeling_codet5p_bimodal.py\n",". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","Downloading pytorch_model.bin: 100%|██████████| 892M/892M [00:04<00:00, 220MB/s]\n","Downloading tokenizer_config.json: 100%|███| 1.34k/1.34k [00:00<00:00, 9.00MB/s]\n","Downloading vocab.json: 100%|████████████████| 511k/511k [00:00<00:00, 11.2MB/s]\n","Downloading merges.txt: 100%|████████████████| 294k/294k [00:00<00:00, 36.9MB/s]\n","Downloading tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 20.8MB/s]\n","Downloading added_tokens.json: 100%|██████████| 59.0/59.0 [00:00<00:00, 393kB/s]\n","Downloading (…)cial_tokens_map.json: 100%|█| 1.03k/1.03k [00:00<00:00, 7.26MB/s]\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","\u001b[32m2023-11-27 13:53:58.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.XLCoST.make_dataset\u001b[0m:\u001b[36m_load_search_dataframe\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mLoading dataframe from data/raw/XLCoST_data/retrieval/nl2code_search/snippet_level/C++/train.jsonl\u001b[0m\n","/kaggle/working/CodeSearcher/src/datasets/XLCoST/make_dataset.py:93: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  dataframe_trunc['code_tokens'] = dataframe_trunc['code_tokens'].apply(_code_tokens_to_str)\n","\u001b[32m2023-11-27 13:54:00.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.XLCoST.make_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mXLCoST C++ train generation=False dataset length: 62839\u001b[0m\n","\u001b[32m2023-11-27 13:54:00.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.XLCoST.make_dataset\u001b[0m:\u001b[36m_load_search_dataframe\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mLoading dataframe from data/raw/XLCoST_data/retrieval/nl2code_search/snippet_level/C++/valid.jsonl\u001b[0m\n","/kaggle/working/CodeSearcher/src/datasets/XLCoST/make_dataset.py:93: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  dataframe_trunc['code_tokens'] = dataframe_trunc['code_tokens'].apply(_code_tokens_to_str)\n","\u001b[32m2023-11-27 13:54:00.594\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.XLCoST.make_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mXLCoST C++ valid generation=False dataset length: 2974\u001b[0m\n","\u001b[32m2023-11-27 13:54:00.594\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.XLCoST.make_dataset\u001b[0m:\u001b[36m_load_search_dataframe\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mLoading dataframe from data/raw/XLCoST_data/retrieval/nl2code_search/snippet_level/C++/test.jsonl\u001b[0m\n","/kaggle/working/CodeSearcher/src/datasets/XLCoST/make_dataset.py:93: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  dataframe_trunc['code_tokens'] = dataframe_trunc['code_tokens'].apply(_code_tokens_to_str)\n","\u001b[32m2023-11-27 13:54:00.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.XLCoST.make_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mXLCoST C++ test generation=False dataset length: 5559\u001b[0m\n","Tokenizing dataset: 100%|███████████████████████| 63/63 [00:12<00:00,  5.04ba/s]\n","Tokenizing dataset: 100%|█████████████████████████| 3/3 [00:00<00:00,  5.53ba/s]\n","Tokenizing dataset: 100%|█████████████████████████| 6/6 [00:00<00:00,  6.15ba/s]\n","Testing the model:   0%|                                | 0/348 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Testing the model: 100%|██████████████████████| 348/348 [01:12<00:00,  4.79it/s]\n","\u001b[32m2023-11-27 13:55:29.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluation\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mTest 0-shot MRR for C++ = 0.13370542563618065\u001b[0m\n"]}],"source":["!cd CodeSearcher/ && export PYTHONPATH=. && python src/models/evaluation.py base \\\n","    --model_name='Salesforce/codet5p-220m-bimodal' \\\n","    --language=\"C++\" "]},{"cell_type":"markdown","metadata":{},"source":["### Seq2Seq 0-shot for C# programming language dataset."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T13:56:21.216865Z","iopub.status.busy":"2023-11-27T13:56:21.216481Z","iopub.status.idle":"2023-11-27T13:58:04.381021Z","shell.execute_reply":"2023-11-27T13:58:04.379855Z","shell.execute_reply.started":"2023-11-27T13:56:21.216829Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","\u001b[32m2023-11-27 13:56:34.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.XLCoST.make_dataset\u001b[0m:\u001b[36m_load_search_dataframe\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mLoading dataframe from data/raw/XLCoST_data/retrieval/nl2code_search/snippet_level/C#/train.jsonl\u001b[0m\n","/kaggle/working/CodeSearcher/src/datasets/XLCoST/make_dataset.py:93: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  dataframe_trunc['code_tokens'] = dataframe_trunc['code_tokens'].apply(_code_tokens_to_str)\n","\u001b[32m2023-11-27 13:56:36.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.XLCoST.make_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mXLCoST C# train generation=False dataset length: 57989\u001b[0m\n","\u001b[32m2023-11-27 13:56:36.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.XLCoST.make_dataset\u001b[0m:\u001b[36m_load_search_dataframe\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mLoading dataframe from data/raw/XLCoST_data/retrieval/nl2code_search/snippet_level/C#/valid.jsonl\u001b[0m\n","/kaggle/working/CodeSearcher/src/datasets/XLCoST/make_dataset.py:93: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  dataframe_trunc['code_tokens'] = dataframe_trunc['code_tokens'].apply(_code_tokens_to_str)\n","\u001b[32m2023-11-27 13:56:36.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.XLCoST.make_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mXLCoST C# valid generation=False dataset length: 2960\u001b[0m\n","\u001b[32m2023-11-27 13:56:36.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.XLCoST.make_dataset\u001b[0m:\u001b[36m_load_search_dataframe\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mLoading dataframe from data/raw/XLCoST_data/retrieval/nl2code_search/snippet_level/C#/test.jsonl\u001b[0m\n","/kaggle/working/CodeSearcher/src/datasets/XLCoST/make_dataset.py:93: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  dataframe_trunc['code_tokens'] = dataframe_trunc['code_tokens'].apply(_code_tokens_to_str)\n","\u001b[32m2023-11-27 13:56:36.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.XLCoST.make_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mXLCoST C# test generation=False dataset length: 5407\u001b[0m\n","Tokenizing dataset: 100%|███████████████████████| 58/58 [00:10<00:00,  5.54ba/s]\n","Tokenizing dataset: 100%|█████████████████████████| 3/3 [00:00<00:00,  5.81ba/s]\n","Tokenizing dataset: 100%|█████████████████████████| 6/6 [00:00<00:00,  6.82ba/s]\n","Testing the model:   0%|                                | 0/338 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Testing the model: 100%|██████████████████████| 338/338 [01:11<00:00,  4.72it/s]\n","\u001b[32m2023-11-27 13:58:02.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluation\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mTest 0-shot MRR for Csharp = 0.13245882310007456\u001b[0m\n"]}],"source":["!cd CodeSearcher/ && export PYTHONPATH=. && python src/models/evaluation.py base \\\n","    --model_name='Salesforce/codet5p-220m-bimodal' \\\n","    --language=\"Csharp\" "]},{"cell_type":"markdown","metadata":{},"source":["### Seq2Seq 0-shot for SQL programming language dataset."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T13:58:04.383834Z","iopub.status.busy":"2023-11-27T13:58:04.383459Z","iopub.status.idle":"2023-11-27T14:00:51.648649Z","shell.execute_reply":"2023-11-27T14:00:51.647322Z","shell.execute_reply.started":"2023-11-27T13:58:04.383796Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Downloading builder script: 100%|██████████| 9.62k/9.62k [00:00<00:00, 7.57MB/s]\n","Downloading and preparing dataset sta_qc/man_sql to /root/.cache/huggingface/datasets/koutch___sta_qc/man_sql/1.0.0/f606addb3f347621e12354120c93505210397cb450814eb6623bf5ecfc8aa807...\n","Downloading data files:   0%|                             | 0/3 [00:00<?, ?it/s]\n","Downloading data: 89.0kB [00:00, 17.5MB/s]                                      \u001b[A\n","Downloading data files:  33%|███████              | 1/3 [00:00<00:00,  4.12it/s]\n","Downloading data:   0%|                             | 0.00/14.6M [00:00<?, ?B/s]\u001b[A\n","Downloading data:  40%|███████▉            | 5.78M/14.6M [00:00<00:00, 57.8MB/s]\u001b[A\n","Downloading data:  93%|██████████████████▌ | 13.6M/14.6M [00:00<00:00, 69.7MB/s]\u001b[A\n","Downloading data: 21.6MB [00:00, 74.4MB/s]                                      \u001b[A\n","Downloading data: 29.5MB [00:00, 76.3MB/s]\u001b[A\n","Downloading data: 37.1MB [00:00, 76.0MB/s]\u001b[A\n","Downloading data: 46.9MB [00:00, 75.2MB/s]\u001b[A\n","Downloading data files:  67%|██████████████       | 2/3 [00:03<00:02,  2.17s/it]\n","Downloading data: 2.77MB [00:00, 34.1MB/s]                                      \u001b[A\n","Downloading data files: 100%|█████████████████████| 3/3 [00:04<00:00,  1.48s/it]\n","Extracting data files: 100%|████████████████████| 3/3 [00:00<00:00, 1616.51it/s]\n","Dataset sta_qc downloaded and prepared to /root/.cache/huggingface/datasets/koutch___sta_qc/man_sql/1.0.0/f606addb3f347621e12354120c93505210397cb450814eb6623bf5ecfc8aa807. Subsequent calls will reuse this data.\n","100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 564.43it/s]\n","Downloading and preparing dataset sta_qc/sca_sql to /root/.cache/huggingface/datasets/koutch___sta_qc/sca_sql/1.0.0/f606addb3f347621e12354120c93505210397cb450814eb6623bf5ecfc8aa807...\n","Downloading data files:   0%|                             | 0/2 [00:00<?, ?it/s]\n","Downloading data:   0%|                             | 0.00/8.75M [00:00<?, ?B/s]\u001b[A\n","Downloading data:  65%|█████████████       | 5.70M/8.75M [00:00<00:00, 57.0MB/s]\u001b[A\n","Downloading data: 13.7MB [00:00, 70.7MB/s]                                      \u001b[A\n","Downloading data: 29.0MB [00:00, 73.7MB/s]\u001b[A\n","Downloading data files:  50%|██████████▌          | 1/2 [00:02<00:02,  2.21s/it]\n","Downloading data:   0%|                             | 0.00/2.00M [00:00<?, ?B/s]\u001b[A\n","Downloading data: 5.24MB [00:00, 48.0MB/s]                                      \u001b[A\n","Downloading data files: 100%|█████████████████████| 2/2 [00:03<00:00,  1.53s/it]\n","Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1311.54it/s]\n","Dataset sta_qc downloaded and prepared to /root/.cache/huggingface/datasets/koutch___sta_qc/sca_sql/1.0.0/f606addb3f347621e12354120c93505210397cb450814eb6623bf5ecfc8aa807. Subsequent calls will reuse this data.\n","100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 693.85it/s]\n","\u001b[32m2023-11-27 13:58:33.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.StaQC.make_dataset\u001b[0m:\u001b[36m_setup_dataset\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mLoading complete\u001b[0m\n","\u001b[32m2023-11-27 13:58:33.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.StaQC.make_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mStaQC SQL train dataset length: 62930\u001b[0m\n","100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 606.64it/s]\n","100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 619.82it/s]\n","\u001b[32m2023-11-27 13:58:36.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.StaQC.make_dataset\u001b[0m:\u001b[36m_setup_dataset\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mLoading complete\u001b[0m\n","\u001b[32m2023-11-27 13:58:36.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.StaQC.make_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mStaQC SQL val dataset length: 6993\u001b[0m\n","100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 665.55it/s]\n","100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 718.08it/s]\n","\u001b[32m2023-11-27 13:58:39.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.StaQC.make_dataset\u001b[0m:\u001b[36m_setup_dataset\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mLoading complete\u001b[0m\n","\u001b[32m2023-11-27 13:58:39.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.StaQC.make_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mStaQC SQL test dataset length: 7770\u001b[0m\n","Tokenizing dataset: 100%|███████████████████████| 63/63 [00:16<00:00,  3.89ba/s]\n","Tokenizing dataset: 100%|█████████████████████████| 7/7 [00:01<00:00,  4.12ba/s]\n","Tokenizing dataset: 100%|█████████████████████████| 8/8 [00:01<00:00,  4.30ba/s]\n","Testing the model:   0%|                                | 0/486 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Testing the model: 100%|██████████████████████| 486/486 [01:45<00:00,  4.61it/s]\n","\u001b[32m2023-11-27 14:00:49.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluation\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mTest 0-shot MRR for SQL = 0.022699319005807066\u001b[0m\n"]}],"source":["!cd CodeSearcher/ && export PYTHONPATH=. && python src/models/evaluation.py base \\\n","    --model_name='Salesforce/codet5p-220m-bimodal' \\\n","    --language=\"SQL\" "]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
